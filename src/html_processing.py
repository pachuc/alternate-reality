#!/usr/bin/env python3
"""
HTML processing module for content extraction, rewriting, and URL manipulation
"""

import re
from typing import Optional, Tuple
from bs4 import BeautifulSoup, NavigableString

from src import llm


def rewrite_urls(content: bytes, content_type: Optional[str]) -> bytes:
    """
    Rewrite URLs in HTML content to go through the proxy.

    Args:
        content: The HTML content as bytes
        content_type: The content type header

    Returns:
        Content with rewritten URLs
    """
    if not content_type or 'text/html' not in content_type:
        return content

    try:
        html = content.decode('utf-8')

        # Replace Wikipedia domain URLs with proxy URLs
        html = re.sub(
            r'https?://([a-z]+\.)?wikipedia\.org',
            'http://localhost:8000',
            html
        )

        # Handle protocol-relative URLs
        html = re.sub(
            r'//([a-z]+\.)?wikipedia\.org',
            '//localhost:8000',
            html
        )

        # Replace Wikimedia URLs
        html = re.sub(
            r'https?://upload\.wikimedia\.org',
            'http://localhost:8000/wikimedia',
            html
        )

        return html.encode('utf-8')
    except Exception as e:
        print(f"Error rewriting URLs: {e}")
        return content


def extract_article_content(html: str) -> Tuple[Optional[str], Optional[str]]:
    """
    Extract the main article content from Wikipedia HTML.

    Args:
        html: The full HTML page

    Returns:
        Tuple of (article_content, article_title) or (None, None) if extraction fails
    """
    try:
        soup = BeautifulSoup(html, 'lxml')

        # Get the article title
        title_element = soup.find('h1', {'class': 'firstHeading'})
        title = title_element.get_text() if title_element else ""

        # Find the main content div - there can be multiple mw-parser-output divs
        # We want the one with actual article content (has paragraphs)
        content_div = None
        all_content_divs = soup.find_all('div', {'class': 'mw-parser-output'})
        for div in all_content_divs:
            # Check if this div has actual content (paragraphs or headers)
            if div.find_all(['p', 'h2', 'h3']):
                content_div = div
                break

        if not content_div:
            return None, None

        # Remove elements we don't want to send to the LLM
        # Remove edit links
        for edit_link in content_div.find_all('span', {'class': 'mw-editsection'}):
            edit_link.decompose()

        # Remove reference markers like [1], [2], etc.
        for ref in content_div.find_all('sup', {'class': 'reference'}):
            ref.decompose()

        # Remove the references section if present
        references_div = content_div.find('div', {'class': 'reflist'})
        if references_div:
            references_div.decompose()

        # Remove navigation boxes
        for navbox in content_div.find_all('div', {'class': 'navbox'}):
            navbox.decompose()

        # Extract just the text content for the LLM
        # We'll get paragraphs, headers, and lists
        text_parts = []

        for element in content_div.find_all(['p', 'h2', 'h3', 'h4', 'ul', 'ol']):
            if element.name in ['h2', 'h3', 'h4']:
                text_parts.append(f"\n{element.get_text().strip()}\n")
            else:
                text = element.get_text().strip()
                if text:  # Only add non-empty content
                    text_parts.append(text)

        article_content = '\n\n'.join(text_parts)
        return article_content, title

    except Exception as e:
        print(f"Error extracting article content: {e}")
        return None, None


def reconstruct_html_with_new_content(html: str, new_content: str) -> str:
    """
    Replace the article content in the HTML with rewritten content.

    Args:
        html: The original HTML page
        new_content: The rewritten article content from LLM

    Returns:
        Modified HTML with new content
    """
    try:
        soup = BeautifulSoup(html, 'lxml')

        # Add a banner indicating this is alternate reality content
        banner_html = """
        <div style="background-color: #fff3cd; border: 2px solid #856404; color: #856404;
                    padding: 15px; margin: 10px 0; border-radius: 5px; text-align: center;">
            <strong>⚠️ Alternate Reality Version</strong><br>
            This article has been rewritten to reflect an alternate history where Germany won World War 2.
            <br><small>Content generated by AI for educational/entertainment purposes only.</small>
        </div>
        """

        # Find the main content div
        content_div = soup.find('div', {'class': 'mw-parser-output'})
        if not content_div:
            return html

        # Clear the existing content (but preserve the div and its attributes)
        content_div.clear()

        # Add the banner first
        banner = BeautifulSoup(banner_html, 'html.parser')
        content_div.append(banner)

        # Split the new content into paragraphs and add them
        paragraphs = new_content.split('\n\n')
        for para_text in paragraphs:
            para_text = para_text.strip()
            if not para_text:
                continue

            # Check if it's a heading (simple heuristic)
            if para_text.startswith('#'):
                # Count the number of # to determine heading level
                level = len(para_text) - len(para_text.lstrip('#'))
                heading_text = para_text.lstrip('#').strip()
                heading = soup.new_tag(f'h{min(level + 1, 6)}')  # h2-h6
                heading.string = heading_text
                content_div.append(heading)
            elif para_text.startswith('='):
                # Alternate heading style
                heading = soup.new_tag('h2')
                heading.string = para_text.strip('=').strip()
                content_div.append(heading)
            else:
                # Regular paragraph
                para = soup.new_tag('p')
                para.string = para_text
                content_div.append(para)

        return str(soup)

    except Exception as e:
        print(f"Error reconstructing HTML: {e}")
        return html


def process_html(content: bytes, content_type: Optional[str], path: str) -> bytes:
    """
    Main function to process HTML content through the rewriting pipeline.

    Args:
        content: The HTML content as bytes
        content_type: The content type header
        path: The request path for context

    Returns:
        Processed HTML content as bytes
    """
    # First, rewrite URLs
    content = rewrite_urls(content, content_type)

    # Only process HTML for LLM rewriting
    if not content_type or 'text/html' not in content_type:
        return content

    # Check if LLM rewriting is enabled
    if not llm.is_enabled():
        return content

    # Skip non-article pages - path might come with or without leading slash
    if not (path.startswith('/wiki/') or path.startswith('wiki/')) or ':' in path:  # Skip special pages like Special:, File:, etc.
        return content

    try:
        html = content.decode('utf-8')

        # Extract article content
        article_content, title = extract_article_content(html)
        if not article_content:
            print(f"Could not extract article content from {path}")
            return content

        print(f"Rewriting article: {title or path}")

        # Get rewritten content from LLM
        new_content = llm.rewrite_content(article_content, title)
        if not new_content:
            print(f"LLM rewriting failed for {path}")
            return content

        # Reconstruct HTML with new content
        modified_html = reconstruct_html_with_new_content(html, new_content)

        return modified_html.encode('utf-8')

    except Exception as e:
        print(f"Error in HTML processing: {e}")
        return content